{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og4WItULsRdY"
      },
      "outputs": [],
      "source": [
        "%pip install gpt4all > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLay9aLOseiG"
      },
      "outputs": [],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuQdhiYTCZRE"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKwSh3FXDDBM"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTkg13MXEt_s"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4LJJq30E1ib",
        "outputId": "fe702999-5a17-42ab-f250-1db04f355156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeIucWoksSTX"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHJS-7WwsSP8"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH_VJA4XsSM1",
        "outputId": "3f014d5c-49c7-4e69-f5d6-bdae4488b27b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "993259it [03:30, 4714.80it/s]\n"
          ]
        }
      ],
      "source": [
        "local_path = (\n",
        "    \"./models/ggml-gpt4all-l13b-snoozy.bin\"  # replace with your desired local file path\n",
        ")\n",
        "\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "# Example model. Check https://github.com/nomic-ai/gpt4all for the latest models.\n",
        "url = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n",
        "\n",
        "# # send a GET request to the URL to download the file. Stream since it's large\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# # open the file in binary mode and write the contents of the response to it in chunks\n",
        "# # This is a large file, so be prepared to wait.\n",
        "with open(local_path, 'wb') as f:\n",
        "     for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "         if chunk:\n",
        "             f.write(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN0RvuoBsSKF",
        "outputId": "7beb8740-9612-466f-db99-7b04bfb9095d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found model file at  ./models/ggml-gpt4all-l13b-snoozy.bin\n",
            "Found model file at  ./models/ggml-gpt4all-l13b-snoozy.bin\n"
          ]
        }
      ],
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callbacks = [StreamingStdOutCallbackHandler()]\n",
        "\n",
        "# Verbose is required to pass to the callback manager\n",
        "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
        "\n",
        "# If you want to use a custom model add the backend parameter\n",
        "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
        "llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bay183jysSHF"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "VLvT84zlsSEF",
        "outputId": "7dd8f26f-f5d2-4a93-a604-fd493dc93a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " First, we need to know when Justin Bieber was born (March 1st, 1994). Then, we can look up which teams played in each of the Super Bowls that took place on or before March 1st, 1994:\n",
            "- The New York Giants won Super Bowl XXI.\n",
            "- The Pittsburgh Steelers won Super Bowl XIII.\n",
            "- The Dallas Cowboys won Super Bowl XII.\n",
            "- The Oakland Raiders won Super Bowl XI.\n",
            "- The Green Bay Packers won Super Bowl X.\n",
            "- The Baltimore Colts (now the Indianapolis Colts) won Super Bowl V.\n",
            "- The Kansas City Chiefs won Super Bowl IV.\n",
            "- The Cleveland Browns won Super Bowl II."
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' First, we need to know when Justin Bieber was born (March 1st, 1994). Then, we can look up which teams played in each of the Super Bowls that took place on or before March 1st, 1994:\\n- The New York Giants won Super Bowl XXI.\\n- The Pittsburgh Steelers won Super Bowl XIII.\\n- The Dallas Cowboys won Super Bowl XII.\\n- The Oakland Raiders won Super Bowl XI.\\n- The Green Bay Packers won Super Bowl X.\\n- The Baltimore Colts (now the Indianapolis Colts) won Super Bowl V.\\n- The Kansas City Chiefs won Super Bowl IV.\\n- The Cleveland Browns won Super Bowl II.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgnw36VCsSBE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import LlamaCppEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAcqRW1mEhnU",
        "outputId": "0173f1b6-b7c8-41b6-d1a1-77e935f6d742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waciPNVhsR-d"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"/content/data_Anlasma_Metni.pdf\")\n",
        "documents = loader.load_and_split()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "t_iPOMidFSrs",
        "outputId": "de35fa5c-3377-41a5-df2a-e03dd49e0c69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb.db.index.hnswlib:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR: The prompt size exceeds the context window size and cannot be processed."
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ERROR: The prompt size exceeds the context window size and cannot be processed.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip8U5gVcDqZr"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6kb6ihsDfJk",
        "outputId": "0b08e90c-36c9-41fe-f14c-f016fe202ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR: The prompt size exceeds the context window size and cannot be processed."
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'output_text': 'ERROR: The prompt size exceeds the context window size and cannot be processed.'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = \"\"\"Compose a final answer by quoting the following quotes from a long document and a question (\"RESOURCES\").\n",
        "If you don't know the answer, say you don't. Don't try to fabricate an answer.\n",
        "ALWAYS include a \"RESOURCES\" section in your answer with the page number, pdf name, and line spacing with the answer.\n",
        "\n",
        "QUESTION: {question}\n",
        "=========\n",
        "{summaries}\n",
        "=========\n",
        "FINAL :\"\"\"\n",
        "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
        "\n",
        "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "query = \"taraflar kimlerdir?\"\n",
        "chain({\"input_documents\": documents, \"question\": query }, return_only_outputs=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
