{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnneP1MybEQE"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip -qqq install git+https://github.com/huggingface/transformers@84a6570 --progress-bar off\n",
        "!pip install -qqq install bitsandbytes --progress-bar off\n",
        "!pip install -qqq install accelerate --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGIyfEh6bHm8"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from transformers import AutoModelForCausaLLM, AutoTokenizer\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EqSDEXcbHg6"
      },
      "outputs": [],
      "source": [
        "def proint_response(response: str) -> str:\n",
        "  response_split = response.split(\"\\n\\n\")\n",
        "  for split in response_split:\n",
        "    print(\"\\n\".join(textwrap.wrap(split,width=110)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYrf9mpYbHdh"
      },
      "outputs": [],
      "source": [
        "Model_name=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(Model_name)\n",
        "\n",
        "model = AutoModelForCausaLLM.from_oretrained(\n",
        "    Model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype= tourch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrTURc2hbHaE"
      },
      "outputs": [],
      "source": [
        "promt = \"what is the meaning of life? use no more than 3 sentences.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQv8l09sbHW_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "with torch.inference_model():\n",
        "  tokens = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=128,\n",
        "      do_sample=False,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      temperature=0.2\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wq8HtynbHTk"
      },
      "outputs": [],
      "source": [
        "output =  tokenizer.decode(tokens[0])\n",
        "output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
